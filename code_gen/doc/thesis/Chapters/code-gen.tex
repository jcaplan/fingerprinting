% Chapter Template

\chapter{Code Generation} % Main chapter title
\label{c:code-gen}


%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
	The code generation framework is designed to automate the porting of Simulink generated control algorithms to the architecture presented in Figure~\ref{f:platform-arch}. 
	The structure of the application being ported follows the assumptions made in the schedulability analysis presented in Chapter~\ref{c:sched}, mainly that tasks are independent and periodic, and that an optimistic and pessimistic WCET have been specified. 
	The hardware and generated code support both DMR with re-execution and TMR for error correction, as well as execution time monitoring. 
	Only a simplified two mode model has been implemented at this time.
	
% 	This chapter begins with a detailed view of the semantics of on-demand redundancy implemented using fingerprinting on the multicore architecture in Figure~\ref{f:platform-arch}. 
% 	The chapter starts with an overview of the main interactions between the system level components, each of which will be discussed in more detail in subsequent sections. 
% 	Finally, the code generation procedure is discussed along with some examples of generated systems.
	
	
	Error detection is implemented using fingerprinting hardware where checksums based on the execution stream are compared to ensure correct operation.
	Local scratchpads are used in combination with memory protection and memory virtualization to ensure that data is quarantined within the sphere of replication until the results have been verified.
	A trusted monitor is responsible for data management and replication across the entire system.
% 	The structure of the application being ported follows the assumptions made in the schedulability analysis presented in Chapter~\ref{c:sched}, mainly that tasks are independent and periodic, and that an optimistic and pessimistic WCET have been specified.
	Code generation requires well defined protocols for the monitor and processing core that can be translated into C templates. 
	Several issues must be addressed for correct operation: deterministic thread execution, fault containment, execution time monitoring, data transfer, and task re-execution in case of fault. 
		
	Figure~\ref{f:correct-op} shows the system level control flow for a correct execution of a DMR replicated task.
	The monitor core (FTC), the processing core, the fingerprint (FP) unit, and the comparator are the main components in the system that implement ODR. 
% 	The following interactions when distributed redundant copies of critical tasks are correctly executed in the system. 
	First the monitor configures the comparator. 
	Then the monitor prepares and sends the data and stack to the scratchpads (SPM) of both processing cores. 
	The monitor then notifies the cores to begin execution of the critical task. 
	Each core notifies its FP unit that a critical task is beginning. 
	The FP units then notify the comparator. 
	The FP units send the checksum to the comparator when a task is complete. 
	When all checksums are received the comparator notifies the monitor of the result. 
	If the execution is correct the monitor then copies back one of the correct scratchpad contents. 




\addfigure{0.6}{imp-correct.pdf}{The main sequence of operations in correct execution of a distributed task on the platform}{f:correct-op}

	Section~\ref{s:mem-arch} provides an overview of the memory architecture. 
	Section~\ref{s:mon} then provides details on the monitor behaviour corresponding to the flow in Figure~\ref{f:correct-op} as well as for the case when a transient error is detected.
	Section~\ref{s:proc-cores} finishes with the implementation details for the processing cores.
	Section~\ref{s:codegen} presents the code generation procedure.
	Section~\ref{s:examples} presents several examples of generated applications.
	
\section{Software Implementation }
\subsection{Memory Achitecture}
\label{s:mem-arch}
	The memory architecture in Figure~\ref{f:mem-part} contains several specialized modules and regions of access to enforce fault containment and deterministic execution in redundant threads. 
	Each core has an MPU to enforce memory protection and uTLB for virtual memory management. 
	As previously mentioned, each core executes on copies of data in the SPM while the original copy remains unaltered in main memory. 
	The MPU is used to ensure that the cores do not access the original copy of the data. 
	In a future implementation, a centralized MPU managed strictly by the monitor would be more dependable. 
	The current MPU is contained in the closed-source Nios code and cannot be modified or extended. 
\addfigure{0.6}{mem-part.pdf}{Memory partition of local and global data space.}{f:mem-part}

% \addfigure{0.6}{sor.pdf}{The memory architecture and the sphere of replication.}{f:sor}
	The shared memory is a region of fast on-chip memory used for inter-core communication. 
	The monitor must pass some information to the processing cores at each task execution such as fingerprint ID (see \cite{ugthesis} for details) and the scratchpad pages allocated to the task (discussed below).
	All data in shared memory is written by only one core to simplify concurrent data accesses. 
% 	The monitor core is responsible for initializing the shared data. % (see Section~\ref{s:inter-core}). 

	The physical address space is partitioned in order to support a simple virtual memory scheme. 
	The monitor is responsible for copying critical data into the scratchpads and back to main memory using a DMA module connected to each scratchpad.
% 	Each DMA is only connected to one local scratchpad. 
% 	There is no ambiguity as each DMA command port has a unique address in the global space. 


% 	On a general note, simplicity has been prioritized over scalability. 
% 	This system sits awkwardly as a demonstration platform as significantly more human resources would be required to complete a robust system. 
% 	As a result, some features are implemented in great detail to highlight specific aspects of design space while others are lacking. 

\subsubsection{Virtual Memory Management}
\label{s:virt-mem}
	Fingerprinting requires the address and data to match for all store instructions which in turn requires that the stack pointer be identical on both cores throughout the execution of the task. 
	Deterministic behaviour is guaranteed by fingerprinting the virtual address and ensuring that both cores use the same virtual address. 
	The uTLB translates the virtual address into the physical address and is programmed by each core according to runtime information provided by the monitor. 		
	
	The uTLB translates addresses in the typical fashion with a table of the upper bits of physical and virtual addresses.
% 	Only the upper bits of the address are compared and replaced. 
	The memory space is separated into pages aligned to the first bit that may be translated.
% 	The uTLB page size is programmable at hardware compile time and is currently set to 4kB. 
	The uTLB setup requires that virtual memory management be handled entirely in software as there are no complex data structures such as page tables or MMU to consult them in the background. 
	
	Several assumptions are enforced by the virtual memory protocol in order to simplify the implementation. 
	Every task has a stack and global data that each consume one 4kB page.
	The uTLB page size is 4kB and each line is statically assigned to translation of the data or stack of a single task.
	The scratchpad is divided evenly into 4kB pages which are dynamically allocated to a task, thus requiring dynamic updating of the translation table values.
% 	These assumptions limit the scalability of the solution insofar as they are not yet reflected in the response time analysis. 

	
	The virtual memory protocol divides each scratchpad into bins according to the page size used by the uTLB. 
	A 16kB scrachpad, for example, contains four bins of 4kB pages.  
	The linker script for each core is updated to reserve one page in main memory for the global data (currently one 4kB page is reserved for \emph{all} global data of fingerprinted tasks rather than on a per-task basis) and one for the stack of each task. 
	The stack size of each task is known statically using profiling information and adding an offset to account for the overhead of context switching and interrupt handling. 
	The virtual address for each stack is assigned statically at design time. 
	The physical address may changes at runtime as the scratchpad location is dynamically assigned and may change as required to support preemption (Section~\ref{s:scratchpad}). 
	The virtual addresses of all stacks point to an unused section of the address space to ensure that no data is corrupted if translation malfunctions or is accidentally turned off.
	
	Each core is initially assigned 200kB of main memory. 
	The stack bins are removed from the end of the main memory allocation by modifying the linker script.
	Listing~\ref{l:virt-mem-link} shows the main memory region of a core has been shortened by 8kB (from 204768 to 196576 on line 5) and two 4kB regions called \texttt{stack\_bin\_x} have been added. 
	Listing~\ref{l:virt-mem-init} shows a condensed version of the relevant startup procedure. 
	First, the stack is declared statically and assigned to the stack bin (line 1). 
	The virtual address for the stack is then set up using the uTLB (lines 4-9). 
	Finally, when creating the task (line 11), the virtual address is used. 
	All references by the RTOS to the stack of this function will use the virtual address. 
	When the monitor signals that the core should begin execution, the interrupt handler will retrieve the runtime information from shared memory and update the memory tables before signalling that the task is ready (Listing~\ref{l:cpu-irq}).


\includecode{virt-mem-link.c}{l:virt-mem-link}{Linker configuration for aligned stack pages}{C}
\includecode{virt-mem-init.c}{l:virt-mem-init}{Starting up system with virtual memory}{C}
\includecode{cpu-irq.c}{l:cpu-irq}{Handling an interrupt from the monitor to start task execution}{C}

	The RTOS kernel has also been modified to support virtual memory management during context switching (Algorithm~\ref{a:mem-manager}). 
	Translation is only enabled for the active task in order to reduce possible sources of error (a higher priority line may have had a conflicting translation in previous iterations). 
	The translation for the starting task $T_i$ is enabled upon entering a context switch for the task being preempted ($T_k$). 
	The translation for $T_k$ cannot be disabled until after the context switch occurs or else an MPU exception will be triggered. 
	The translation is managed such that $T_i$ and $T_k$ will not clash when both are active at the same time.

\begin{algorithm}
	\Upon{Start task $T_i$}{
		Retrieve translation for data and stack pages from shared memory\;
	}
	\Upon{Enter context switch}{
		\If{Preempted task $T_k$ has translation activated}{
			Flag $T_k$ to disable translation\;
		}
		Enable translation for next task $T_i$\;
	}
	\Upon{Exit context switch}{
		\If{$\exists T_k$ flagged for disable}{
			Disable $T_k$ translation\;
		}
	}
	\caption{Memory management procedure during context switch.}
	\label{a:mem-manager}
\end{algorithm}




\subsubsection{Memory Protection}
\label{s:mem-prot}
	Each core has its own operating system and must initialize its stack when creating the new task using the built-in RTOS functions. 
	Once complete, the MPU is enabled before the RTOS starts executing tasks. 
	The MPU catches null pointers, as well as writes to the memory space of other cores or its own critical memory space. 
	This provides some added protection against faults in the non-replicated sections once startup has successfully completed.

\subsection{Monitor}
\label{s:mon}
	
% The monitor tasks must run on the fault tolerant core. They consist of:
% \begin{itemize}
%   \item setting up task parameters and initializing all critical task models
% %   \item managing dataflow between tasks
%   \item intercore task communication
%   \item retrieving valid data when critical tasks execute correctly on other resources
%   \item restarting tasks in the case of a transient fault
%   \item managing the global data space
%   \item organizing coordinated virtual memory management and memory protection between cores to achieve correct fingerprinting
% \end{itemize}


\subsubsection{Replication Services}
	The primary responsibility of the monitor core is to initiate execution of critical tasks on the processing cores and to take action in case a fault is detected on either core. 
	The current implementation only supports tasks that execute on a regular period. 
	
	Task start times are tracked with a software timer provided by the RTOS. 
	Every time the system clock triggers an interrupt, the monitor updates and checks a table containing information on all critical tasks that require fingerprinting and takes appropriate action (Algorithm~\ref{a:repos-start}).
	Each task has a statically defined period and a counter associated with it. 
	The counter is incremented every millisecond and the task is started when the counter reaches the task period.

\begin{algorithm}
\Upon{Clock tick (1 ms)}{
	\For{All tasks $T_i$}{
		Check for overflow\;
		\If{for $T_i$.count == $T_i$.period}{
			$T_i$.count = 1\;
			Start task $T_i$\;
		}\Else {
			$T_i$.count++\;
		}
	}
}
\Upon{Start task $T_i$}{
	\tcc{Possibility of replacing or customizing in future for any task}
	Initiate DMA transfer\;
}
\caption{Starting critical tasks from monitor on each clock tick.}
\label{a:repos-start}
\end{algorithm}	

	The data structure for each task contains a pointer to a \emph{start hook} that allows the user to define a custom startup routine for each critical task.
% 	\emph{Hooks} are a commonly used technique to implement behaviour somewhat similar to an abstract method in object oriented programming. 
% 	(hooks are user provided functions that are called from library code at strategic locations with well defined purposes). 
	Users can replace empty hooks with their own code to add extra functionality to the library without modifying the complicated library code directly. 
	The default start hook initiates DMA transfer from main memory to the processing core scratchpads and updates the state of the monitors internal data structures.
	A future addition to a start hook might include retrieving data from a sensor and updating the task data before initiating DMA transfer.		



\subsubsection{DMA transfer}
\label{s:dma-trans}
	The default start hook called by the replication services upates internal state and initiates DMA transfer to each core. 
	Algorithm~\ref{a:dma} runs from a high priority thread in the monitor to initiate DMA transfers of critical data to and from processing core scratchpads.
	In the case of TMR, the algorithm ensures that the updated copy of data is retrieved from one of the cores with matching results.
	The DMA thread uses a message queue rather than a semaphore to ensure that only one task can trigger the thread at a time.
	Future work on FPGA development should include the implementation of a coarse grained time triggered network protocol. 
	The monitor does not initiate more than one DMA transfer at a time to minimize interference with the aim of simplifying future expansion to the response time analysis and to facilitate integration of a time triggered communication layer.

\begin{algorithm}

\Upon{Start task $T_i$}{
	\If{Stack is not in scratchpad}{
		Request scratchpad destination\;
		Send stack to each core\;
		Send data to each core\;
	}\Else{
		Send data to each core\;
	}
	Update shared memory and notify both cores of start\;
}
\Upon{End task $T_i$}{
	Copy data back from one successful core\;
}
\caption{DMA transfer of critical data.}
\label{a:dma}
\end{algorithm}

DMA transfers are simplified by placing all global data and input arguments required by a critical task in a single container struct. 
The data container ensures that all data is located in a contiguous region of memory and simplifies the implementation of generic DMA functions that only require the task index in the critical table as input.


\subsubsection{Scratchpad Management}
\label{s:scratchpad}

	A centralized runtime mechanism has been implemented in the monitor to manage dynamic scratchpad allocation for critical tasks.
	The scratchpad allocation can not be determined statically because of the dynamic preemptive RMS scheduling. 
	The scratchpad is in fact partitioned into two physical sections to prevent DMA traffic for the \emph{next} task from interfering with the execution of the current task. 
	
	Algorithm~\ref{a:scratchpad} shows the steps taken during scratchpad allocation. 
	First, a scratchpad $S_j$ must be selected that does not hold the data for a currently executing task. 
	A page $P_k$ is then selected from $S_j$ that has either not been used (invalid) or holds data for the lowest priority task.

	If a task has been preempted (is still active but not currently executing) then its data may not be evicted under the current implementation, effectively limiting the well defined behaviour of the system to four critical tasks per core (two scratchpads with four pages each).
	Future work could explore the performance of different replacement policies as well as well as more robust mechanisms to support more critical tasks per core.
	
\begin{algorithm}
\SetKwFunction{AssignScratchpad}{AssignScratchpad}
\Fn{\AssignScratchpad{$\pi_i$,$T_i$}}{
	invalidLine $\leftarrow$ false\;
	priority $\leftarrow$ null\;
	page $\leftarrow$ null\;
	scratchpad  $\leftarrow$ null\;
	\For{Each scratchpad $S_j$}{
		\If{No critical task in progress on $\pi_i$ or other scratchpad is being used}{
			\For{Each page in scratchpad $P_k$}{
				\If{Page has not been used yet}{
					priority $\leftarrow$ $T_i$ priority\;
					scratchpad $\leftarrow$ $S_j$\;
					page $\leftarrow$ $P_k$\;
					invalidLine $\leftarrow$ true\;
					break\;
				}
				\ElseIf{Page does not contain in progress (preempted) task}{
						\If{$T_i$ priority > priority}{
							priority $\leftarrow$ $T_i$ priority\;
							scratchpad $\leftarrow$ $S_j$\;
							page $\leftarrow$ $P_k$\;
						}
				}
				
			}
			\If{invalidLine}{
				break\;
			}
		}
	}
	
	Assign $S_j$ and $P_k$ to $T_i$\;
}
\caption{Scratchpad assignment of for task $T_i$ on core $\pi_i$}
\label{a:scratchpad}
\end{algorithm}

\subsubsection{Restarting Tasks and Cores}

	Hardware facilities are provided to restart an entire core if necessary. 
	However, the current model assumes that it will only be necessary to restart a task.
	The comparator is a hardware module capable of sending an interrupt to the monitor. 
	If a task has failed in the comparison stage then the monitor resets the state for that task and resends the data and stack to the scratchpad.
	The method is sufficient because the error is assumed not to affect the RTOS kernel which implies that execution will successfully wind back up the call stack to the top level and reach the same wait point in its loop regardless of the error.
	Watchdog timers and memory exception prototcols would need to be implemented in order to tolerate more severe types of errors (null pointers or other access exceptions and infinite loops).

% \subsection{Fingerprint Management}

% The page size of the uTLB is programmable at hardware compile time and is set to 4kB. The stack pointer must match for the entire function for execution fingerprints to match. So long as the address data

\subsection{Processing cores}
\label{s:proc-cores}
\subsubsection{Running Critical Tasks on Processing Cores}

	Each critical task is assumed to be called from a single function that takes all global data (including persistent state) as arguments. 
	Listing~\ref{l:crit-task-wrapper} shows a simplified example wrapper (the setting of some global RTOS flags has been omitted). 
	As in any periodic task in a C based RTOS, the function consists of an infinite loop and waits on a semaphore to begin each loop iteration. 
	First, the runtime monitor which measures execution time is notified that the task is starting a new iteration. 
	Then the core resets all of its callee saved registers to 0. 
	This is required for deterministic execution because the contents of callee saved registers are history dependent and may spill onto the stack. 

\includecode{crit-task-wrapper.c}{l:crit-task-wrapper}{Example wrapper function for critical task}{C}

	Both cores must execute the same critical code since the return addresses will be pushed onto the stack and must match in order for fingerprints to match. 
	However, virtual memory has not yet been implemented for the instruction bus. 
	As a result, critical code is compiled only for one processing core and this core must pass the function pointer to the other core so that they both execute the same code. 
	One small problem arises because the Nios II architecture includes a global pointer similar to MIPS which raises a potential p.
	The Nios compiler does not guarantee that compiled code does not use the global pointer (the option is specified but unimplemented). 
	However, shared code using Matlab's reusable function option should not generate global pointer references as there are no direct references to static objects. 
	It is possible to temporarily change the global pointer of one core to match anothers as discussed in prior work \cite{ugthesis}.


\subsubsection{Runtime Monitor}

	The runtime monitor maintains a an execution time counter for each execution of all tasks on every core and is responsible for the mode change in case of an overrun.
	AMC response time analysis assumes that some runtime monitoring facilities exist to catch when a task exceeds its $C(LO)$ (Section~\ref{s:mcfts}). 
	Only the simple two mode behaviour has been implemented and all LO criticality tasks are dropped at the mode change. 
	Reverse mode changes (restarting the LO tasks) have also been omitted.

	The runtime monitor must be notified at the beginning and end of each iteration for each task (shown in Listing~\ref{l:crit-task-wrapper} lines 9 and 23). 
	The RTOS kernel must also be modified to update the runtime monitor tables when a context switch occurs. 
	The runtime monitor uses a hardware timer rather than rely on the OS software clock to improve precision.
	
	Algorithm~\ref{a:run-mon} shows how the runtime monitor keeps track of each task.
	A single timer is reset when a task starts or a context switch occurs.
	The value of the hardware timer is added to a counter for each task at every clock tick and when a task is preempted.

\begin{algorithm}
	\Upon{Start task $T_i$}{
		Put $T_i$ in table\;
		Mark $T_i$ as running\;
		Reset $T_i$ counter\;
		Reset the timer\;
	}
	\Upon{End task $T_i$}{
		Mark $T_i$ as not running\;
	}
	\Upon{Pause task $T_i$}{
		\If{$T_i$ in table}{
			Add timer value to $T_i$\;
		}
	}
	\Upon{Unpause task $T_i$}{
		Reset the timer\;
	}
	\Upon{System clock interrupt (every ms)}{
		\For{Running task $T_i$}{
			Add timer value to $T_i$\;
			Reset the timer\;
			\If{C >= C(LO)}{
				Drop all LO tasks\;
			}
		}
	}
	\caption{Runtime monitoring of execution time.}
	\label{a:run-mon}
\end{algorithm}


\section{Code Generation}
\label{s:codegen}

	The code generation (CG) stage generates the embedded C code that implements the semantics described in previous sections. 
	Referring back to Figure~\ref{f:tool-arch}, CG is the final stage after code profiling (Chapter~\ref{c:prof}) and mapping and scheduling (Chapter~\ref{c:sched}).  
	The basic CG flow is shown in Algorithm~\ref{l:cg-flow}. 
	Each step in the flow will be briefly discussed.

\begin{algorithm}
\SetKwFunction{GenerateCode}{GenerateCode}

\Fn{\GenerateCode{Configuration}}{
	\If{No BSP found}{
		Generate BSPs\;
	}
	\If{Profiling required}{
		Profile binary code\;
	}
	\If{Mapping required}{
		Find mapping\;
	}
	Copy source code to app directories\;
	Parse source code\;
	Generate scripts\;
	Calculate stack bins\;
	Generate monitor main\;
	Generate processing cores main\;
}
\caption{Basic code generation flow.}
\label{l:cg-flow}
\end{algorithm}


\subsection{BSP Generation}
	Altera provides a command line interface (Nios SBT) for the generation and customization of Nios II BSPs based on hardware platforms specified using Quartus QSYS. 
	CG generates and runs bash scripts that use the Nios SBT and provides a paper-trail for the user so that they can manually update and re-run the scripts in case some modification is required.
	Some BSP modifications are not easily supported by the Nios SBT and must be manually configured by the shell script including: Updating the version of uC/OS-II RTOS to be compatible with the Imperas OS debugging tools, replacing customized files in the generated bare-metal drivers (HAL), modifying certain errors in generated files due to known bugs in the Nios SBT (especially the linker script and ``system.h'' file, and adding drivers for all the custom hardware and software libraries. Appendix~\ref{A:ConfigScripts} contains example scripts.

\subsection{Code Profiling}

	A sample project is required to profile the code for each task computation. 
	A new application project is configured for one of the generated BSPs using an example main program generated by Simulink using each function. 
	The tool is configured to recognize the default naming patterns used by Simulink. 
	Future work could provide full customization of naming patterns to match the Simulink options. 
	The profiling proceeds according to Chapter~\ref{c:prof} once the project has been compiled and the gcc tool \texttt{nios2-elf-objdump} has been used to generate assembly code and extract annotations.

\subsection{Mapping and Scheduling}
	The mapping and scheduling tool is automatically configured to test the default platform and task set using information from the configuration file and optionally the profiling results.
	The program exits if no mapping solution is found.

\subsection{Parsing Source Code}

	The Simulink generated source code must also be parsed to determine if the functions have default parameters and/or persistent state. 
	These data structures require special handling in the setup of the data structures because there is a containing \emph{model struct} for which the pointers to these variables must be set. 
	Listing~\ref{l:pointer-setup} shows the generated code for setting up the struct.
	
\includecode{pointer-setup.c}{l:pointer-setup}{Setting up data for radar tracking function}{C}
	

	Simulink generates the \texttt{RT\_MODEL\_*} data struct which contains the pointers to parameters and state as well as a struct for inputs (\texttt{EXT\_U\_*}) and outputs (\texttt{EXT\_Y\_*}). 
	The \texttt{*\_initialize} function called on line 13 is generated by Simulink. The update pointer function is called twice - once before initializing the model struct and once after. 
	The pointer to the state variable initially points to the physical address because the monitor will modify the variable in the \texttt{initialize} function. 
	Afterwards, the pointer is set to the virtual address that the processing core will use.

\subsection{Generating Application}

	The application requires a Makefile based on the source code required for each function running on that core. First the source code for each core is copied into a folder. The application Makefile is then generated using Nios SBT. Appendix~\ref{A:ConfigScripts} contains a sample script for generating the Makefile. 

\subsection{Stack Bin Calculation}

	The stack and data for critical task are placed in separate memory regions from the rest of the application. 
	The linker script is not modified until the final mapping is known in order to minimize the amount of memory that must be partitioned.
	Appendix~\ref{A:ConfigScripts} contains a sample script.

\subsection{Generating Main Files}

	Algorithm~\ref{l:cg-monitor} shows the steps for generating the main monitor file. 
	The steps for generating code for the processing core are the same however the implementation differs for several. 
	The code generation follows a simple template based approach where specific values must be substituted into generic templates based on the application configuration.
	Appendix~\ref{A:moncode} contains a sample of generated monitor code and processing core.


\begin{algorithm}
\SetKwFunction{GenerateMonitor}{GenerateMonitor}

\Fn{\GenerateMonitor{}}{
	Generate list of files to include\;
	Generate global variable declarations\;
	Generate task stack declarations\;
	Generate initializion code for the execution time monitor\;
	Generate interrupt handlers\;
	Generate task wrappers\;
	Generate initializion code for memory manager\;
	Generate MPU initialization code\;
	Generate main function\;
}
\caption{Basic code generation flow.}
\label{l:cg-monitor}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Examples}
\label{s:examples} 

	Appendix~\ref{A:ConfigFile} contains a configuration file for a code generation project. 
	Two functions have been generated using Simulink that execute some for loop. Table~\ref{t:config} summarizes the parameters.
	
	
\subsection{Mixed Criticality System with Two Processing Cores}	

\begin{table}[h]
\caption{Example application}
\centering

	\begin{tabular}{@{}llllllll@{}}
	Function & Per. & Crit. & Prio. & Mapping & Max Stack & C(LO) & C(HI) 	 \\
	
	\toprule
	T0 & 30 & HI & 2 & cpu0,cpu1 & 80 & 1600004 & 2400006 \\
	T1 & 30 & LO & 1 & cpu0 & 80 & 1600035 & - \\
	\end{tabular}

\label{t:config}
\end{table}
	

	The generated code is executed on the OVP virtual platform using Imperas debugging tools. The Imperas debugger analyzes the RTOS and generates a waveform representing the activity of the various tasks. 
	An excerpt of the simulation is shown in Figure~\ref{f:sim1}.
	The DMA task on the monitor executes once at 542 ms and takes almost no time since the delay for DMA transfer is not modelled.
	The two copies of the HI task execute in parallel. The LO task interrupts the HI task on \emph{cpu0} however the response time remains under 30 ms.
\addfigure{0.55}{sim1.png}{Simulation of sample program}{f:sim1}

	We can observe a mode change by setting C(LO) to 9 ms for the task \emph{T1} because it has been configured to run between 8-12 ms. 
	Figure~\ref{f:sim2} shows that once \emph{T1} executes for 9 ms it is dropped while the HI task continues to run.
	 
\addfigure{0.52}{sim2.png}{LO task is dropped after $C > C(LO)$}{f:sim2}
		
	Finally, in the case of an error, the task must be re-executed. 
	The periods of \emph{T0} and \emph{T1} are extended to 60 ms for this example as the system in Table~\ref{t:config} is not schedule in the present of faults. 
	\emph{T0} had a response time of 24 ms in Figure~\ref{f:sim1} and cannot complete in 30 ms if re-execution is required).
	A transient fault is simulated by pausing the simulation and changing the value in a general purpose register in Figure~\ref{f:sim3}. 
	The DMA task runs again as it re-sends the stack and data to the scratchpads.
	The LO task does not execute after the fault as a mode change occurs.
	
		 
\addfigure{0.45}{sim3.png}{HI task is re-executed after fault is detected}{f:sim3}

\subsection{Four Processing Core System}

\begin{table}[h]
\caption{Example application}
\centering

	\begin{tabular}{@{}llllllll@{}}
	Function & Per. & Crit. & Prio. & Mapping & Max Stack & C(LO) & C(HI) 	 \\
	
	\toprule
	T0 & 90 & HI & 2 & cpu0,cpu1 & 80 & 1600004 & 2400006 \\
	T1 & 150 & HI & 3 & cpu0,cpu1 & 80 & 1600004 & 2400006 \\
	T2 & 75 & HI & 4 & cpu0,cpu2 & 80 & 1600004 & 2400006 \\
	T3 & 75 & HI & 5 & cpu2,cpu3 & 80 & 1600004 & 2400006 \\
	T4 & 75 & HI & 6 & cpu3,cpu1 & 80 & 1600004 & 2400006 \\
	T5 & 60 & LO & 1 & cpu0 & 80 & 1600035 & - \\
	\end{tabular}

\label{t:dcc}
\end{table}

	Table~\ref{t:dcc} shows a larger set of tasks mapped to a system with four processing cores. 
	Different pairs of cores are coupled for different tasks. 
	Figure~\ref{f:sim4core} shows an exerpt of the platform execution waveform. 
	Note that the timing of replicas can be quite different on each core.
	\emph{T2}, for instance, begins execution at 210ms on \emph{cpu2}.
	The other replica on \emph{cpu0} does not begin until 230ms since other high priority tasks must finish 
	and it is preempted by \emph{T0} during execution. 
	The final response time exceeds 40ms which respects the 75ms deadline.
	
	The DMA task on the monitor is considerably more active as the number of tasks in the system increases.
	The execution time of the DMA task would be considerably longer relative to the task deadline on an FPGA and suggests that future work will require modelling of communication overhead in the schedulability analysis.
% 	The waveform also does not depict interrupt activity which will increase with the number of tasks.
	
	
\addfigure{0.5}{sim_4core.png}{Code generation supports up to four cores.}{f:sim4core}

\begin{table}[h]
\caption{Example application}
\centering
	\begin{tabular}{@{}llllllll@{}}
	Function & Per. & Crit. & Prio. & Mapping & Max Stack & C(LO) & C(HI) 	 \\
	
	\toprule
	T0 & 50 & HI & 6 & cpu0,cpu1,cpu2 & 80 & 1600004 & 2400006 \\
	T1 & 75 & HI & 5 & cpu2,cpu3 & 80 & 1600004 & 2400006 \\
	\end{tabular}
	
	
\label{t:tmr}
\end{table}

	Figure~\ref{f:simtmr} demonstrates a mix of DMR and TMR in the same system for the task set in Table~\ref{t:tmr}. 
	Once again, the third replica of \emph{T0} on \emph{cpu2} does not execute at the same time as on the other two cores because it is preempted by a higher priority task.

\addfigure{0.42}{sim_tmr.png}{DMR and TMR in same system.}{f:simtmr}





