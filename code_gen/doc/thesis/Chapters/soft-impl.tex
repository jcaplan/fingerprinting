% Chapter Template

\chapter{Software Implementation} % Main chapter title

\label{c:soft-impl} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
	This chapter will give a detailed view of the semantics of the fingerprint based multicore architecture in Figure~\ref{f:platform-arch}.
	The target architecture provides hardware mechanisms for support of on-demand redundancy. 		
	Errors are detected in replicated threads using fingerprinting hardware to monitor bus traffic. 	
	Checksums based on the execution stream are compared to ensure correct operation.
	Local scratchpads are used in combination with memory protection and memory virtualization to ensure that data is quarantined within the sphere of replication until the results have been verified.
	A trusted monitor is responsible for data management and replication across the entire system.
	Code generation currently supports TMR and DMR.
	
	Code generation (Chapter~\ref{c:code-gen}) requires well defined protocols for the monitor and processing core that can be translated into C templates. 
	Several issues must be addressed for correct operation: deterministic thread execution, fault containment, execution time monitoring, data transfer, and task re-execution in case of fault. 
		
	Figure~\ref{f:correct-op} shows the system level control flow for a correct execution of a DMR replicated task.
	The monitor core (FTC), the processing core, the fingerprint (FP) unit, and the comparator are the main components in the system that implement ODR. 
% 	The following interactions when distributed redundant copies of critical tasks are correctly executed in the system. 
	First the monitor configures the comparator. 
	Then the monitor prepares and sends the data and stack to the scratchpads (SPM) of both processing cores. 
	The monitor then notifies the cores to begin execution of the critical task. 
	Each core notifies its FP unit that a critical task is beginning. 
	The FP units then notify the comparator. 
	The FP units send the checksum to the comparator when a task is complete. 
	When all checksums are received the comparator notifies the monitor of the result. 
	If the execution is correct the monitor then copies back one of the correct scratchpad contents. 




\addfigure{0.6}{imp-correct.pdf}{The main sequence of operations in correct execution of a distributed task on the platform}{f:correct-op}

	Section~\ref{s:mem-arch} provides an overview of the memory architecture. 
	Section~\ref{s:mon} then provides details on the monitor behaviour corresponding to the flow in Figure~\ref{f:correct-op} as well as for the case when a transient error is detected.
	Section~\ref{s:proc-cores} finishes with the implementation details for the processing cores.

\section{Memory Achitecture}
\label{s:mem-arch}
	The memory architecture in Figure~\ref{f:mem-part} contains several specialized modules and regions of access to enforce fault containment and deterministic execution in redundant threads. 
	Each core has an MPU to enforce memory protection and uTLB for virtual memory management. 
	As previously mentioned, each core executes on copies of data in the SPM while the original copy remains unaltered in main memory. 
	The MPU is used to ensure that the cores do not access the original copy of the data. 
	In a future implementation, a centralized MPU managed strictly by the monitor would be more dependable. 
	The current MPU is contained in the closed-source Nios code and cannot be modified or extended. 
\addfigure{0.6}{mem-part.pdf}{Memory partition of local and global data space.}{f:mem-part}

% \addfigure{0.6}{sor.pdf}{The memory architecture and the sphere of replication.}{f:sor}
	The shared memory is a region of fast on-chip memory used for inter-core communication. 
	The monitor must pass some information to the processing cores at each task execution such as fingerprint ID (see \cite{ugthesis} for details) and the scratchpad pages allocated to the task (discussed below).
	All data in shared memory is written by only one core to simplify concurrent data accesses. 
% 	The monitor core is responsible for initializing the shared data. % (see Section~\ref{s:inter-core}). 

	The physical address space is partitioned in order to support a simple virtual memory scheme. 
	The monitor is responsible for copying critical data into the scratchpads and back to main memory using a DMA module connected to each scratchpad.
% 	Each DMA is only connected to one local scratchpad. 
% 	There is no ambiguity as each DMA command port has a unique address in the global space. 


% 	On a general note, simplicity has been prioritized over scalability. 
% 	This system sits awkwardly as a demonstration platform as significantly more human resources would be required to complete a robust system. 
% 	As a result, some features are implemented in great detail to highlight specific aspects of design space while others are lacking. 

\subsection{Virtual Memory Management}
\label{s:virt-mem}
	Fingerprinting requires the address and data to match for all store instructions which in turn requires that the stack pointer be identical on both cores throughout the execution of the task. 
	Deterministic behaviour is guaranteed by fingerprinting the virtual address and ensuring that both cores use the same virtual address. 
	The uTLB translates the virtual address into the physical address and is programmed by each core according to runtime information provided by the monitor. 		
	
	The uTLB translates addresses in the typical fashion with a table of the upper bits of physical and virtual addresses.
% 	Only the upper bits of the address are compared and replaced. 
	The memory space is separated into pages aligned to the first bit that may be translated.
% 	The uTLB page size is programmable at hardware compile time and is currently set to 4kB. 
	The uTLB setup requires that virtual memory management be handled entirely in software as there are no complex data structures such as page tables or MMU to consult them in the background. 
	
	Several assumptions are enforced by the virtual memory protocol in order to simplify the implementation. 
	Every task has a stack and global data that each consume one 4kB page.
	The uTLB page size is 4kB and each line is statically assigned to translation of the data or stack of a single task.
	The scratchpad is divided evenly into 4kB pages which are dynamically allocated to a task, thus requiring dynamic updating of the translation table values.
% 	These assumptions limit the scalability of the solution insofar as they are not yet reflected in the response time analysis. 

	
	The virtual memory protocol divides each scratchpad into bins according to the page size used by the uTLB. 
	A 16kB scrachpad, for example, contains four bins of 4kB pages.  
	The linker script for each core is updated to reserve one page in main memory for the global data (currently one 4kB page is reserved for \emph{all} global data of fingerprinted tasks rather than on a per-task basis) and one for the stack of each task. 
	The stack size of each task is known statically using profiling information and adding an offset to account for the overhead of context switching and interrupt handling. 
	The virtual address for each stack is assigned statically at design time. 
	The physical address may changes at runtime as the scratchpad location is dynamically assigned and may change as required to support preemption (Section~\ref{s:scratchpad}). 
	The virtual addresses of all stacks point to an unused section of the address space to ensure that no data is corrupted if translation malfunctions or is accidentally turned off.
	
	Each core is initially assigned 200kB of main memory. 
	The stack bins are removed from the end of the main memory allocation by modifying the linker script.
	Listing~\ref{l:virt-mem-link} shows the main memory region of a core has been shortened by 8kB (from 204768 to 196576 on line 5) and two 4kB regions called \texttt{stack\_bin\_x} have been added. 
	Listing~\ref{l:virt-mem-init} shows a condensed version of the relevant startup procedure. 
	First, the stack is declared statically and assigned to the stack bin (line 1). 
	The virtual address for the stack is then set up using the uTLB (lines 4-9). 
	Finally, when creating the task (line 11), the virtual address is used. 
	All references by the RTOS to the stack of this function will use the virtual address. 
	When the monitor signals that the core should begin execution, the interrupt handler will retrieve the runtime information from shared memory and update the memory tables before signalling that the task is ready (Listing~\ref{l:cpu-irq}).


\includecode{virt-mem-link.c}{l:virt-mem-link}{Linker configuration for aligned stack pages}{C}
\includecode{virt-mem-init.c}{l:virt-mem-init}{Starting up system with virtual memory}{C}
\includecode{cpu-irq.c}{l:cpu-irq}{Handling an interrupt from the monitor to start task execution}{C}

	The RTOS kernel has also been modified to support virtual memory management during context switching (Algorithm~\ref{a:mem-manager}). 
	Translation is only enabled for the active task in order to reduce possible sources of error (a higher priority line may have had a conflicting translation in previous iterations). 
	The translation for the starting task $T_i$ is enabled upon entering a context switch for the task being preempted ($T_k$). 
	The translation for $T_k$ cannot be disabled until after the context switch occurs or else an MPU exception will be triggered. 
	The translation is managed such that $T_i$ and $T_k$ will not clash when both are active at the same time.

\begin{algorithm}
	\Upon{Start task $T_i$}{
		Retrieve translation for data and stack pages from shared memory\;
	}
	\Upon{Enter context switch}{
		\If{Preempted task $T_k$ has translation activated}{
			Flag $T_k$ to disable translation\;
		}
		Enable translation for next task $T_i$\;
	}
	\Upon{Exit context switch}{
		\If{$\exists T_k$ flagged for disable}{
			Disable $T_k$ translation\;
		}
	}
	\caption{Memory management procedure during context switch.}
	\label{a:mem-manager}
\end{algorithm}




\subsection{Memory Protection}
\label{s:mem-prot}
	Each core has its own operating system and must initialize its stack when creating the new task using the built-in RTOS functions. 
	Once complete, the MPU is enabled before the RTOS starts executing tasks. 
	The MPU catches null pointers, as well as writes to the memory space of other cores or its own critical memory space. 
	This provides some added protection against faults in the non-replicated sections once startup has successfully completed.

\section{Monitor}
\label{s:mon}
	
% The monitor tasks must run on the fault tolerant core. They consist of:
% \begin{itemize}
%   \item setting up task parameters and initializing all critical task models
% %   \item managing dataflow between tasks
%   \item intercore task communication
%   \item retrieving valid data when critical tasks execute correctly on other resources
%   \item restarting tasks in the case of a transient fault
%   \item managing the global data space
%   \item organizing coordinated virtual memory management and memory protection between cores to achieve correct fingerprinting
% \end{itemize}


\subsection{Replication Services}
	The primary responsibility of the monitor core is to initiate execution of critical tasks on the processing cores and to take action in case a fault is detected on either core. 
	The current implementation only supports tasks that execute on a regular period. 
	
	Task start times are tracked with a software timer provided by the RTOS. 
	Every time the system clock triggers an interrupt, the monitor updates and checks a table containing information on all critical tasks that require fingerprinting and takes appropriate action (Algorithm~\ref{a:repos-start}).
	Each task has a statically defined period and a counter associated with it. 
	The counter is incremented every millisecond and the task is started when the counter reaches the task period.

\begin{algorithm}
\Upon{Clock tick (1 ms)}{
	\For{All tasks $T_i$}{
		Check for overflow\;
		\If{for $T_i$.count == $T_i$.period}{
			$T_i$.count = 1\;
			Start task $T_i$\;
		}\Else {
			$T_i$.count++\;
		}
	}
}
\Upon{Start task $T_i$}{
	\tcc{Possibility of replacing or customizing in future for any task}
	Initiate DMA transfer\;
}
\caption{Starting critical tasks from monitor on each clock tick.}
\label{a:repos-start}
\end{algorithm}	

	The data structure for each task contains a pointer to a \emph{start hook} that allows the user to define a custom startup routine for each critical task.
% 	\emph{Hooks} are a commonly used technique to implement behaviour somewhat similar to an abstract method in object oriented programming. 
% 	(hooks are user provided functions that are called from library code at strategic locations with well defined purposes). 
	Users can replace empty hooks with their own code to add extra functionality to the library without modifying the complicated library code directly. 
	The default start hook initiates DMA transfer from main memory to the processing core scratchpads and updates the state of the monitors internal data structures.
	A future addition to a start hook might include retrieving data from a sensor and updating the task data before initiating DMA transfer.		



\subsection{DMA transfer}
\label{s:dma-trans}
	The default start hook called by the replication services upates internal state and initiates DMA transfer to each core. 
	Algorithm~\ref{a:dma} runs from a high priority thread in the monitor to initiate DMA transfers of critical data to and from processing core scratchpads.
	In the case of TMR, the algorithm ensures that the updated copy of data is retrieved from one of the cores with matching results.
	The DMA thread uses a message queue rather than a semaphore to ensure that only one task can trigger the thread at a time.
	Future work on FPGA development should include the implementation of a coarse grained time triggered network protocol. 
	The monitor does not initiate more than one DMA transfer at a time to minimize interference with the aim of simplifying future expansion to the response time analysis and to facilitate integration of a time triggered communication layer.

\begin{algorithm}

\Upon{Start task $T_i$}{
	\If{Stack is not in scratchpad}{
		Request scratchpad destination\;
		Send stack to each core\;
		Send data to each core\;
	}\Else{
		Send data to each core\;
	}
	Update shared memory and notify both cores of start\;
}
\Upon{End task $T_i$}{
	Copy data back from one successful core\;
}
\caption{DMA transfer of critical data.}
\label{a:dma}
\end{algorithm}

DMA transfers are simplified by placing all global data and input arguments required by a critical task in a single container struct. 
The data container ensures that all data is located in a contiguous region of memory and simplifies the implementation of generic DMA functions that only require the task index in the critical table as input.


\subsection{Scratchpad Management}
\label{s:scratchpad}

	A centralized runtime mechanism has been implemented in the monitor to manage dynamic scratchpad allocation for critical tasks.
	The scratchpad allocation can not be determined statically because of the dynamic preemptive RMS scheduling. 
	The scratchpad is in fact partitioned into two physical sections to prevent DMA traffic for the \emph{next} task from interfering with the execution of the current task. 
	
	Algorithm~\ref{a:scratchpad} shows the steps taken during scratchpad allocation. 
	First, a scratchpad $S_j$ must be selected that does not hold the data for a currently executing task. 
	A page $P_k$ is then selected from $S_j$ that has either not been used (invalid) or holds data for the lowest priority task.

	If a task has been preempted (is still active but not currently executing) then its data may not be evicted under the current implementation, effectively limiting the well defined behaviour of the system to four critical tasks per core (two scratchpads with four pages each).
	Future work could explore the performance of different replacement policies as well as well as more robust mechanisms to support more critical tasks per core.
	
\begin{algorithm}
\SetKwFunction{AssignScratchpad}{AssignScratchpad}
\Fn{\AssignScratchpad{$\pi_i$,$T_i$}}{
	invalidLine $\leftarrow$ false\;
	priority $\leftarrow$ null\;
	page $\leftarrow$ null\;
	scratchpad  $\leftarrow$ null\;
	\For{Each scratchpad $S_j$}{
		\If{No critical task in progress on $\pi_i$ or other scratchpad is being used}{
			\For{Each page in scratchpad $P_k$}{
				\If{Page has not been used yet}{
					priority $\leftarrow$ $T_i$ priority\;
					scratchpad $\leftarrow$ $S_j$\;
					page $\leftarrow$ $P_k$\;
					invalidLine $\leftarrow$ true\;
					break\;
				}
				\ElseIf{Page does not contain in progress (preempted) task}{
						\If{$T_i$ priority > priority}{
							priority $\leftarrow$ $T_i$ priority\;
							scratchpad $\leftarrow$ $S_j$\;
							page $\leftarrow$ $P_k$\;
						}
				}
				
			}
			\If{invalidLine}{
				break\;
			}
		}
	}
	
	Assign $S_j$ and $P_k$ to $T_i$\;
}
\caption{Scratchpad assignment of for task $T_i$ on core $\pi_i$}
\label{a:scratchpad}
\end{algorithm}

\subsection{Restarting Tasks and Cores}

	Hardware facilities are provided to restart an entire core if necessary. 
	However, the current model assumes that it will only be necessary to restart a task.
	The comparator is a hardware module capable of sending an interrupt to the monitor. 
	If a task has failed in the comparison stage then the monitor resets the state for that task and resends the data and stack to the scratchpad.
	The method is sufficient because the error is assumed not to affect the RTOS kernel which implies that execution will successfully wind back up the call stack to the top level and reach the same wait point in its loop regardless of the error.
	Watchdog timers and memory exception prototcols would need to be implemented in order to tolerate more severe types of errors (null pointers or other access exceptions and infinite loops).

% \subsection{Fingerprint Management}

% The page size of the uTLB is programmable at hardware compile time and is set to 4kB. The stack pointer must match for the entire function for execution fingerprints to match. So long as the address data

\section{Processing cores}
\label{s:proc-cores}
\subsection{Running Critical Tasks on Processing Cores}

	Each critical task is assumed to be called from a single function that takes all global data (including persistent state) as arguments. 
	Listing~\ref{l:crit-task-wrapper} shows a simplified example wrapper (the setting of some global RTOS flags has been omitted). 
	As in any periodic task in a C based RTOS, the function consists of an infinite loop and waits on a semaphore to begin each loop iteration. 
	First, the runtime monitor which measures execution time is notified that the task is starting a new iteration. 
	Then the core resets all of its callee saved registers to 0. 
	This is required for deterministic execution because the contents of callee saved registers are history dependent and may spill onto the stack. 

\includecode{crit-task-wrapper.c}{l:crit-task-wrapper}{Example wrapper function for critical task}{C}

	Both cores must execute the same critical code since the return addresses will be pushed onto the stack and must match in order for fingerprints to match. 
	However, virtual memory has not yet been implemented for the instruction bus. 
	As a result, critical code is compiled only for one processing core and this core must pass the function pointer to the other core so that they both execute the same code. 
	One small problem arises because the Nios II architecture includes a global pointer similar to MIPS which raises a potential p.
	The Nios compiler does not guarantee that compiled code does not use the global pointer (the option is specified but unimplemented). 
	However, shared code using Matlab's reusable function option should not generate global pointer references as there are no direct references to static objects. 
	It is possible to temporarily change the global pointer of one core to match anothers as discussed in prior work \cite{ugthesis}.


\subsection{Runtime Monitor}

	The runtime monitor maintains a an execution time counter for each execution of all tasks on every core and is responsible for the mode change in case of an overrun.
	AMC response time analysis assumes that some runtime monitoring facilities exist to catch when a task exceeds its $C(LO)$ (Section~\ref{s:mcfts}). 
	Only the simple two mode behaviour has been implemented and all LO criticality tasks are dropped at the mode change. 
	Reverse mode changes (restarting the LO tasks) have also been omitted.

	The runtime monitor must be notified at the beginning and end of each iteration for each task (shown in Listing~\ref{l:crit-task-wrapper} lines 9 and 23). 
	The RTOS kernel must also be modified to update the runtime monitor tables when a context switch occurs. 
	The runtime monitor uses a hardware timer rather than rely on the OS software clock to improve precision.
	
	Algorithm~\ref{a:run-mon} shows how the runtime monitor keeps track of each task.
	A single timer is reset when a task starts or a context switch occurs.
	The value of the hardware timer is added to a counter for each task at every clock tick and when a task is preempted.

\begin{algorithm}
	\Upon{Start task $T_i$}{
		Put $T_i$ in table\;
		Mark $T_i$ as running\;
		Reset $T_i$ counter\;
		Reset the timer\;
	}
	\Upon{End task $T_i$}{
		Mark $T_i$ as not running\;
	}
	\Upon{Pause task $T_i$}{
		\If{$T_i$ in table}{
			Add timer value to $T_i$\;
		}
	}
	\Upon{Unpause task $T_i$}{
		Reset the timer\;
	}
	\Upon{System clock interrupt (every ms)}{
		\For{Running task $T_i$}{
			Add timer value to $T_i$\;
			Reset the timer\;
			\If{C >= C(LO)}{
				Drop all LO tasks\;
			}
		}
	}
	\caption{Runtime monitoring of execution time.}
	\label{a:run-mon}
\end{algorithm}
 



% 
% \begin{table}[h]
% \caption{Address Space Partition}
% \centering
% 
% 	\begin{tabular}{@{}ll@{}}
% 	\toprule
% 	Local Peripherals &0x8000000-0x8FFFFFF 	 \\
% 	Scratchpad &0x8420000-0x8427FFF 			 \\
% 	Shared Memory &0x2500000-0x25003FF		\\
% 	Main Memory &0x0000000-0x0095FFF		\\
% 	\end{tabular}
% 
% \label{t:virt-mem}
% \end{table}
	



% what is shared memory? What goes there? how is it used ?
% 
% What is the scratchpad address ?
% 
% what is the distribution of addresses local vs. global ?
% 
% what is the memory virtualization strategy ?






